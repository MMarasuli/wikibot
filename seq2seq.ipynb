{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBAJLpSFUHFdsKPnpOQgw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tohpedo/wikibot/blob/seq2seq/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRXlwTfbROJq"
      },
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "yM5bxWWTRXZE",
        "outputId": "60d4123d-7fa2-4129-c4b1-1a44507fcab4"
      },
      "source": [
        "# base\n",
        "\n",
        "base = pd.read_csv(\"/content/drive/MyDrive/ACIT/database.csv\")[:1000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f12602e2a586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ACIT/database.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ACIT/database.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-EqgbgtRaX8"
      },
      "source": [
        "# define function to clean the text \n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = re.sub(r\"\\n\", \"\",  text)\n",
        "    text = re.sub(r\"[-()]\", \"\", text)\n",
        "    text = re.sub(r\"\\.\", \" . \", text)\n",
        "    text = re.sub(r\"\\!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\?\", \" ? \", text)\n",
        "    text = re.sub(r\"\\,\", \" , \", text)\n",
        "    text = re.sub(r\"\\\"\", \" \\\" \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1b33vALSnp2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFGQOiwqRd7v"
      },
      "source": [
        "# base processing \n",
        "\n",
        "base = base.dropna()\n",
        "\n",
        "# defining lenghts of q's and a's\n",
        "\n",
        "q_text = []\n",
        "\n",
        "for line in base.question:\n",
        "    q_text.append(clean_text(line))\n",
        "\n",
        "q_len = []\n",
        "for line in q_text:\n",
        "    q_len.append(len(line.split()))\n",
        "\n",
        "max_q_length = max(q_len)\n",
        "\n",
        "a_text = []\n",
        "for line in base.text:\n",
        "    a_text.append(clean_text(line))\n",
        "\n",
        "a_len = []\n",
        "for line in a_text:\n",
        "    a_len.append(len(line.split()))\n",
        "\n",
        "# plus two due to tokens\n",
        "max_a_length = max(a_len) + 2\n",
        "\n",
        "# adding tokens to answers\n",
        "for i in range(len(a_text)):\n",
        "    a_text[i]  = \"<BOS> \" + a_text[i] + \" <EOS>\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_OiNMKkRiup"
      },
      "source": [
        "# vocab and tokenizing\n",
        "# making vocab\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "vocab_size = 10000\n",
        "tokenizer = Tokenizer(num_words = vocab_size, lower=False, filters=\"\")\n",
        "tokenizer.fit_on_texts(q_text + a_text)\n",
        "dictionary = tokenizer.word_index\n",
        "\n",
        "word2token = {}\n",
        "token2word = {}\n",
        "\n",
        "for k, v in dictionary.items():\n",
        "    if v < vocab_size:\n",
        "        word2token[k] = v\n",
        "        token2word[v] = k\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "vocab_size = len(word2token) + 1\n",
        "\n",
        "# tokenizing sentences\n",
        "\n",
        "encoder_seq = tokenizer.texts_to_sequences(q_text)\n",
        "decoder_seq = tokenizer.texts_to_sequences(a_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvxGxQUGRmdF"
      },
      "source": [
        "# padding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "encoder_input = pad_sequences(encoder_seq, \n",
        "                              maxlen = max_q_length,\n",
        "                              padding = \"post\",\n",
        "                              truncating= \"post\")\n",
        "\n",
        "decoder_input = pad_sequences(decoder_seq, \n",
        "                              maxlen = max_a_length,\n",
        "                              padding = \"post\",\n",
        "                              truncating= \"post\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU5yUpOBRoo7"
      },
      "source": [
        "# formating decoder output\n",
        "for i in range(len(decoder_seq)):\n",
        "    decoder_seq[i] =  decoder_seq[i][1:]\n",
        "    \n",
        "# pad with 0\n",
        "padded_answers = pad_sequences(decoder_seq,\n",
        "                               maxlen = max_a_length,\n",
        "                               padding = \"post\")\n",
        "decoder_output = to_categorical(padded_answers, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6PIKSr_RsT5"
      },
      "source": [
        "#deleting non-necessary variables\n",
        "del(a_len, a_text, decoder_seq, dictionary, encoder_seq, i, k, line, padded_answers, q_len, q_text,  v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcQu8WEZRu7E"
      },
      "source": [
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n",
        "\n",
        "# encoder will be used to capture space-dependent relations between words from the questions\n",
        "# about 200 neurons needed \n",
        "\n",
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embeding = Embedding(vocab_size, 250, mask_zero=True)\n",
        "enc_embeding = enc_embeding(enc_inputs)\n",
        "enc_lstm = LSTM(250,  return_state=True)\n",
        "enc_outputs, h, c = enc_lstm(enc_embeding)\n",
        "enc_states = [h, c]\n",
        "\n",
        "# decoder will be used to capture space-dependent relations between words from the answers using encoder's internal state as a context\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(vocab_size, 250, mask_zero=True)\n",
        "dec_embedding = dec_embedding(dec_inputs)\n",
        "dec_lstm = LSTM(250, return_state=True, return_sequences=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state = enc_states)\n",
        "\n",
        "# decoder is connected to the output Dense layer\n",
        "dec_dense = Dense(vocab_size, activation = \"softmax\")\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "# output of this network will look like this:\n",
        "# y_true = [0.05, 0.95, 0...]\n",
        "# and expected one-hot encoded output like this:\n",
        "# y_pred = [0, 1, 0...]\n",
        "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\n",
        "model.summary()\n",
        "\n",
        "model.fit([encoder_input, decoder_input],\n",
        "          decoder_output,\n",
        "          batch_size = 30,\n",
        "          epochs = 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD3A50sZR6cz"
      },
      "source": [
        "# =============================================================================\n",
        "# Bot\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "\n",
        "def make_inference_models():\n",
        "    # two inputs for the state vectors returned by encoder\n",
        "    dec_state_input_h = Input(shape=(250,))\n",
        "    dec_state_input_c = Input(shape=(250,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    # these state vectors are used as an initial state \n",
        "    # for LSTM layer in the inference decoder\n",
        "    # third input is the Embedding layer as explained above   \n",
        "    dec_outputs, h, c = dec_lstm(dec_embedding,\n",
        "                                    initial_state=dec_states_inputs)\n",
        "    dec_states = [h, c]\n",
        "    # Dense layer is used to return OHE predicted word\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs=[dec_outputs] + dec_states)\n",
        "   \n",
        "    # single encoder input is a question, represented as a sequence \n",
        "    # of integers padded with zeros\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "   \n",
        "    return enc_model, dec_model\n",
        "\n",
        "enc_model, dec_model = make_inference_models()\n",
        "\n",
        "def str_to_tokens(sentence: str):\n",
        "\n",
        "    sentence = clean_text(sentence)\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != \"\":\n",
        "            tokens_list.append(result)\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen = max_q_length,\n",
        "                         padding = \"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIFUxqKBSGeN"
      },
      "source": [
        "# =============================================================================\n",
        "# chatting loop\n",
        "# =============================================================================\n",
        "\n",
        "for _ in range(5):\n",
        "    # encode the input sequence into state vectors\n",
        "    states_values = enc_model.predict(str_to_tokens(input(\"user : \")))\n",
        "    # start with a target sequence of size 1 - word 'start'   \n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index[\"<BOS>\"]\n",
        "    stop_condition = False\n",
        "    decoded_translation = \"\"\n",
        "    while not stop_condition:\n",
        "        # feed vectors and word for prediction\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq] \n",
        "                                              + states_values)         \n",
        "        # sample the next word using these predictions\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        # append the sampled word to the target sequence\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != \"<EOS>\":\n",
        "                    decoded_translation += \" {}\".format(word)\n",
        "                sampled_word = word\n",
        "        # repeat until <EOS> or lenght\n",
        "        if sampled_word == \"<EOS>\" \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > max_a_length:\n",
        "            stop_condition = True\n",
        "        # prepare next iteration\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "    print(\"chatbot: \" + decoded_translation)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}