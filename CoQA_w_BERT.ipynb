{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoQA w BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-fHoYbCAMd7",
        "outputId": "0a0a485f-54c9-4692-da2b-03b680c47ead"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 48.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "haisvnaGAUZt",
        "outputId": "ec755fdb-e4c3-4a66-b479-4f8e5df655c7"
      },
      "source": [
        "coqa = pd.read_json('http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json')\n",
        "coqa.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'gutenberg', 'id': '3urfvvm165iantk...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   version                                               data\n",
              "0        1  {'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...\n",
              "1        1  {'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...\n",
              "2        1  {'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...\n",
              "3        1  {'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...\n",
              "4        1  {'source': 'gutenberg', 'id': '3urfvvm165iantk..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtKTD2GCAUld"
      },
      "source": [
        "del coqa[\"version\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkOSkykpAen3"
      },
      "source": [
        "#required columns in our dataframe\n",
        "cols = [\"text\",\"question\",\"answer\"]\n",
        "#list of lists to create our dataframe\n",
        "comp_list = []\n",
        "for index, row in coqa.iterrows():\n",
        "    for i in range(len(row[\"data\"][\"questions\"])):\n",
        "        temp_list = []\n",
        "        temp_list.append(row[\"data\"][\"story\"])\n",
        "        temp_list.append(row[\"data\"][\"questions\"][i][\"input_text\"])\n",
        "        temp_list.append(row[\"data\"][\"answers\"][i][\"input_text\"])\n",
        "        comp_list.append(temp_list)\n",
        "new_df = pd.DataFrame(comp_list, columns=cols) \n",
        "#saving the dataframe to csv file for further loading\n",
        "new_df.to_csv(\"CoQA_data.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dJz90eXAie9"
      },
      "source": [
        "data = pd.read_csv(\"CoQA_data.csv\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElJIlglNAmaQ"
      },
      "source": [
        "#print the number of question and answer pairs in dataset\n",
        "print(\"Number of question and answers: \", len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pozIapCAuUf"
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gjnzK3cAz-2"
      },
      "source": [
        "random_num = np.random.randint(0,len(data))\n",
        "question = data[\"question\"][random_num]\n",
        "text = data[\"text\"][random_num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4EKTeKYA30_"
      },
      "source": [
        "input_ids = tokenizer.encode(question, text)\n",
        "print(\"The input has a total of {} tokens.\".format(len(input_ids)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk9tS7dTA6pu"
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    print('{:8}{:8,}'.format(token,id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya0JB1QDA9Sl"
      },
      "source": [
        "#first occurence of [SEP] token\n",
        "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "print(\"SEP token index: \", sep_idx)\n",
        "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
        "num_seg_a = sep_idx+1\n",
        "print(\"Number of tokens in segment A: \", num_seg_a)\n",
        "#number of tokens in segment B (text)\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "print(\"Number of tokens in segment B: \", num_seg_b)\n",
        "#creating the segment ids\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "#making sure that every input token has a segment id\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3ArC0o6BAbh"
      },
      "source": [
        "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
        "output = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cBMHGhiBDdp"
      },
      "source": [
        "#tokens with highest start and end scores\n",
        "answer_start = torch.argmax(output.start_logits)\n",
        "answer_end = torch.argmax(output.end_logits)\n",
        "if answer_end >= answer_start:\n",
        "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
        "else:\n",
        "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
        "    \n",
        "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
        "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42nEvsVzBGAt"
      },
      "source": [
        "answer = tokens[answer_start]\n",
        "for i in range(answer_start+1, answer_end+1):\n",
        "    if tokens[i][0:2] == \"##\":\n",
        "        answer += tokens[i][2:]\n",
        "    else:\n",
        "        answer += \" \" + tokens[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxoUWKkwBKHe"
      },
      "source": [
        "def question_answer(question, text):\n",
        "    \n",
        "    #tokenize question and text as a pair\n",
        "    input_ids = tokenizer.encode(question, text)\n",
        "    \n",
        "    #string version of tokenized ids\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    \n",
        "    #segment IDs\n",
        "    #first occurence of [SEP] token\n",
        "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "    #number of tokens in segment A (question)\n",
        "    num_seg_a = sep_idx+1\n",
        "    #number of tokens in segment B (text)\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    \n",
        "    #list of 0s and 1s for segment embeddings\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "    \n",
        "    #model output using input_ids and segment_ids\n",
        "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
        "    \n",
        "    #reconstructing the answer\n",
        "    answer_start = torch.argmax(output.start_logits)\n",
        "    answer_end = torch.argmax(output.end_logits)\n",
        "    if answer_end >= answer_start:\n",
        "        answer = tokens[answer_start]\n",
        "        for i in range(answer_start+1, answer_end+1):\n",
        "            if tokens[i][0:2] == \"##\":\n",
        "                answer += tokens[i][2:]\n",
        "            else:\n",
        "                answer += \" \" + tokens[i]\n",
        "                \n",
        "    if answer.startswith(\"[CLS]\"):\n",
        "        answer = \"Unable to find the answer to your question.\"\n",
        "    \n",
        "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eanka1G_BSa_"
      },
      "source": [
        "text = \"\"\"New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \\\"Motown 25,\\\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \\\"Clyde\\\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. \"The legacy that [Jackson] left behind is bigger than life for me,\\\" Orange said. \\\"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\\\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.\"\"\"\n",
        "question = \"Where was the Auction held?\"\n",
        "question_answer(question, text)\n",
        "#original answer from the dataset\n",
        "print(\"Original answer:\\n\", data.loc[data[\"question\"] == question][\"answer\"].values[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKmLbFdbBXD_"
      },
      "source": [
        "text = input(\"Please enter your text: \\n\")\n",
        "question = input(\"\\nPlease enter your question: \\n\")\n",
        "while True:\n",
        "    question_answer(question, text)\n",
        "    \n",
        "    flag = True\n",
        "    flag_N = False\n",
        "    \n",
        "    while flag:\n",
        "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
        "        if response[0] == \"Y\":\n",
        "            question = input(\"\\nPlease enter your question: \\n\")\n",
        "            flag = False\n",
        "        elif response[0] == \"N\":\n",
        "            print(\"\\nBye!\")\n",
        "            flag = False\n",
        "            flag_N = True\n",
        "            \n",
        "    if flag_N == True:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}